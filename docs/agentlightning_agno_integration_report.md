# B√°o C√°o T√≠ch H·ª£p AgentLightning v·ªõi AGNO

## üìã T·ªïng Quan

B√°o c√°o n√†y tr√¨nh b√†y chi ti·∫øt v·ªÅ vi·ªác t√≠ch h·ª£p **AgentLightning** (framework training t·ª´ Microsoft) v·ªõi **AGNO** (framework x√¢y d·ª±ng AI agents) ƒë·ªÉ th·ª±c hi·ªán training agents v·ªõi thu·∫≠t to√°n **APO (Automatic Prompt Optimization)**.

---


### 1.1 AgentLightning Framework

**AgentLightning** l√† framework t·ª´ Microsoft ƒë·ªÉ training v√† c·∫£i thi·ªán AI agents th√¥ng qua reinforcement learning.

#### T√≠nh NƒÉng Ch√≠nh

- **Zero Code Change Integration**: T√≠ch h·ª£p v·ªõi agents hi·ªán c√≥ m√† kh√¥ng c·∫ßn thay ƒë·ªïi code nhi·ªÅu
- **Multiple Algorithms**: H·ªó tr·ª£ RL, APO, Supervised Fine-tuning
- **Framework Agnostic**: Ho·∫°t ƒë·ªông v·ªõi LangChain, AutoGen, CrewAI, ho·∫∑c custom agents
- **Continuous Learning**: V√≤ng l·∫∑p training li√™n t·ª•c ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t

#### Ki·∫øn Tr√∫c AgentLightning

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          AgentLightning System               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Trainer    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ LightningStore‚îÇ         ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ    ‚îÇ (Tasks/Traces)‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Algorithm  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Agent       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (APO/RL)   ‚îÇ    ‚îÇ  Executor    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                                    ‚îÇ
‚îÇ         ‚ñº                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ  ‚îÇ Updated    ‚îÇ                             ‚îÇ
‚îÇ  ‚îÇ Resources  ‚îÇ                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Thu·∫≠t To√°n APO (Automatic Prompt Optimization)

### 2.1 Kh√°i Ni·ªám & C·∫•u H√¨nh Th·ª±c T·∫ø
**APO (Automatic Prompt Optimization)** l√† k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a prompt t·ª± ƒë·ªông s·ª≠ d·ª•ng LLM ƒë·ªÉ "ph√™ b√¨nh" v√† "ch·ªânh s·ª≠a" ch√≠nh n√≥.

Trong d·ª± √°n n√†y, thu·∫≠t to√°n APO ƒë∆∞·ª£c c·∫•u h√¨nh c·ª• th·ªÉ nh∆∞ sau (trong `rollout.py`):
- **Gradient Model**: `gpt-4o-mini` (ƒê√≥ng vai tr√≤ Critic - Ph√¢n t√≠ch l·ªói sai).
- **Edit Model**: `gpt-4o-mini` (ƒê√≥ng vai tr√≤ Editor - S·ª≠a l·∫°i prompt d·ª±a tr√™n ph√™ b√¨nh).
- **C∆° ch·∫ø**: S·ª≠ d·ª•ng "Textual Gradients" - thay v√¨ ƒë·∫°o h√†m s·ªë h·ªçc nh∆∞ Deep Learning truy·ªÅn th·ªëng, APO t·∫°o ra c√°c ƒë·∫°o h√†m vƒÉn b·∫£n (l·ªùi ph√™ b√¨nh) ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng vi·ªác s·ª≠a ƒë·ªïi.

### 2.2 V√≠ D·ª• Th·ª±c T·∫ø T·ª´ D·ªØ Li·ªáu Ch·∫°y
D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• minh h·ªça s·ª± thay ƒë·ªïi c·ªßa prompt qua qu√° tr√¨nh hu·∫•n luy·ªán th·ª±c t·∫ø (tr√≠ch xu·∫•t t·ª´ `best_prompts.json`):

**Giai ƒëo·∫°n 1: Kh·ªüi ƒë·ªông (Iteration 1)**
H·ªá th·ªëng b·∫Øt ƒë·∫ßu v·ªõi m·ªôt prompt c∆° b·∫£n ho·∫∑c prompt ti·∫øng Vi·ªát do ng∆∞·ªùi d√πng cung c·∫•p.
> **Prompt:** *"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√∫p t√¥i gi·∫£i quy·∫øt th·∫Øc m·∫Øc"*  
> **Reward:** 1.0 (Kh·ªüi ƒëi·ªÉm)

**Giai ƒëo·∫°n 2: T·ªëi ∆∞u h√≥a (Iteration 2 - M√¥ ph·ªèng)**
Sau khi ch·∫°y qua c√°c tasks th·ª±c t·∫ø, h·ªá th·ªëng nh·∫≠n th·∫•y prompt tr√™n qu√° ng·∫Øn v√† thi·∫øu h∆∞·ªõng d·∫´n c·ª• th·ªÉ khi g·∫∑p c√°c c√¢u h·ªèi to√°n h·ªçc ph·ª©c t·∫°p.
> **Critique (Gradient):** "Prompt hi·ªán t·∫°i qu√° chung chung. Agent thi·∫øu h∆∞·ªõng d·∫´n v·ªÅ vi·ªác gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc (Chain-of-Thought) v√† ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi r√µ r√†ng."

> **Optimized Prompt (Candidate):**
> *"You are a helpful math assistant. Solve math problems step by step. When given a calculation task, use the available tools. Always show your work and provide the final answer clearly. Be accurate and precise in your calculations."*

**K·∫øt qu·∫£:**
C√°c phi√™n b·∫£n prompt sau khi t·ªëi ∆∞u (English version) th∆∞·ªùng c√≥ c·∫•u tr√∫c r√µ r√†ng h∆°n, h∆∞·ªõng d·∫´n agent th·ª±c hi·ªán c√°c b∆∞·ªõc suy lu·∫≠n c·ª• th·ªÉ, t·ª´ ƒë√≥ n√¢ng cao ƒë·ªô ch√≠nh x√°c khi gi·∫£i quy·∫øt c√°c t√°c v·ª• ph·ª©c t·∫°p.

## 3. Ki·∫øn Tr√∫c Hi·ªán T·∫°i C·ªßa D·ª± √Ån

### 3.1 C·∫•u Tr√∫c Th∆∞ M·ª•c

```
d:\Python\Agent\
‚îú‚îÄ‚îÄ core/                    # AGNO agent core
‚îÇ   ‚îî‚îÄ‚îÄ agno_memory.db      # Session v√† trace storage
‚îú‚îÄ‚îÄ training/               # AgentLightning training
‚îÇ   ‚îú‚îÄ‚îÄ train.py           # Main orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Training configuration
‚îÇ   ‚îú‚îÄ‚îÄ setup.py           # Infrastructure setup
‚îÇ   ‚îú‚îÄ‚îÄ data/              # Data preparation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_to_dataset.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ engine/            # Training execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_executor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer_factory.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rollout.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grader.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ infrastructure.py
‚îÇ       ‚îú‚îÄ‚îÄ prompt_manager.py
‚îÇ       ‚îú‚îÄ‚îÄ result_saver.py
‚îÇ       ‚îî‚îÄ‚îÄ otlp.py
‚îú‚îÄ‚îÄ app/                    # Streamlit UI
‚îî‚îÄ‚îÄ main.py                # Entry point
```

### 3.2 Lu·ªìng Training Hi·ªán T·∫°i

```python
# T·ª´ train.py
def main():
    # 1. Initialize (Dependencies + Config + Infrastructure)
    agent_config, training_config, db = initialize_training()
    
    # 2. Prepare datasets
    train_dataset, val_dataset = prepare_datasets(training_config)
    
    # 3. Execute training
    trainer, initial_prompt = run_training(
        training_config, agent_config, db,
        train_dataset, val_dataset
    )
    
    # 4. Save results
    save_training_results(trainer, initial_prompt, training_config)
```

---

## 4. T√≠ch H·ª£p OpenTelemetry & Tracing

### 4.1 Vai Tr√≤ C·ªßa OpenTelemetry
**OpenTelemetry (OTEL)** ƒë∆∞·ª£c s·ª≠ d·ª•ng trong d·ª± √°n ƒë·ªÉ thu th·∫≠p c√°c **traces** (d·∫•u v·∫øt th·ª±c thi) c·ªßa Agent. Trong thu·∫≠t to√°n **APO**, c√°c traces n√†y c·ª±c k·ª≥ quan tr·ªçng v√¨:
- Cung c·∫•p d·ªØ li·ªáu th√¥ v·ªÅ qu√° tr√¨nh suy nghƒ© (reasoning) v√† g·ªçi c√¥ng c·ª• (tool calls).
- Cho ph√©p **LLM Critic** ph√¢n t√≠ch ch√≠nh x√°c Agent ƒë√£ "sai" ·ªü b∆∞·ªõc n√†o ƒë·ªÉ t·∫°o ra Critique ch·∫•t l∆∞·ª£ng.
- L∆∞u tr·ªØ l·ªãch s·ª≠ th·ª±c thi ƒë·ªÉ ƒë·ªëi chi·∫øu hi·ªáu su·∫•t c·ªßa Agent.

### 4.2 C·∫•u H√¨nh OTLP Exporter
D·ª± √°n s·ª≠ d·ª•ng `OTLPSpanExporter` ƒë·ªÉ g·ª≠i d·ªØ li·ªáu traces ƒë·∫øn **Lightning Store**.

**File c·∫•u h√¨nh:** [otlp.py](file:///home/lamquy/Project/Agent_demo/training/utils/otlp.py)

```python
def setup_otlp_exporter(endpoint: str, service_name: str):
    # Kh·ªüi t·∫°o TracerProvider
    resource = Resource(attributes={SERVICE_NAME: service_name})
    provider = TracerProvider(resource=resource)
    
    # C·∫•u h√¨nh Exporter g·ª≠i ƒë·∫øn AgentLightning Store
    otlp_exporter = OTLPSpanExporter(endpoint=endpoint)
    provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
    
    # Thi·∫øt l·∫≠p l√†m Global Tracer
    trace.set_tracer_provider(provider)
```

### 4.3 Theo D√µi Traces (Dashboard)
B·∫°n c√≥ th·ªÉ theo d√µi c√°c traces n√†y tr·ª±c quan th√¥ng qua dashboard b·∫±ng l·ªánh:
```bash
agl store --port 4747
```

---

## 5. C√°c B∆∞·ªõc T√≠ch H·ª£p AgentLightning v·ªõi AGNO

### 5.1 Ki·∫øn Tr√∫c T√≠ch H·ª£p

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                AGNO + AgentLightning                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ AGNO Agent   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ APO Algorithm  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ (Executor)   ‚îÇ         ‚îÇ                ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚îÇ                         ‚ñ≤                     ‚îÇ
‚îÇ         ‚îÇ Traces                  ‚îÇ Updated             ‚îÇ
‚îÇ         ‚ñº                         ‚îÇ Prompts             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ Lightning    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Trainer        ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ Store        ‚îÇ         ‚îÇ                ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚îÇ                                               ‚îÇ
‚îÇ         ‚ñº                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                      ‚îÇ
‚îÇ  ‚îÇ Training     ‚îÇ                                      ‚îÇ
‚îÇ  ‚îÇ Dataset      ‚îÇ                                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng

#### a. C√†i ƒê·∫∑t Dependencies

```bash
# ƒê√£ c√≥ trong requirements.txt
pip install agentlightning
pip install agno
pip install opentelemetry-api
pip install opentelemetry-sdk
```

#### b. C·∫•u H√¨nh Environment Variables

```bash
# .env
OPENAI_API_KEY=your-api-key
AGENTOPS_API_KEY=your-agentops-key  # Optional, for tracing
AGENTOPS_LOG_LEVEL=DEBUG  # Optional, for debugging
```

### 5.3 B∆∞·ªõc 2: Wrap AGNO Agent cho AgentLightning

#### T·∫°o Lightning-Compatible Agent Wrapper

```python
# training/agent_wrapper.py
from agentlightning import Agent
from agno import Agno
from opentelemetry import trace

class AgnoLightningAgent(Agent):
    """Wrapper ƒë·ªÉ AGNO agent t∆∞∆°ng th√≠ch v·ªõi AgentLightning."""
    
    def __init__(self, agent_config):
        super().__init__()
        
        # Kh·ªüi t·∫°o AGNO agent
        self.agno_agent = Agno(
            model=agent_config.get("model", "gpt-4o-mini"),
            tools=agent_config.get("tools", []),
            memory=agent_config.get("memory", True),
            reasoning=True,
            markdown=True
        )
        
        # Prompt template c√≥ th·ªÉ tune ƒë∆∞·ª£c
        self.system_prompt = agent_config.get(
            "system_prompt",
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√∫p t√¥i gi·∫£i quy·∫øt th·∫Øc m·∫Øc"
        )
    
    def run(self, task: str) -> dict:
        """
        Ch·∫°y agent tr√™n task v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ v·ªõi tracing.
        
        AgentLightning s·∫Ω:
        1. G·ªçi run() v·ªõi task
        2. Thu th·∫≠p traces
        3. T√≠nh reward
        4. D√πng APO ƒë·ªÉ c·∫£i thi·ªán system_prompt
        """
        tracer = trace.get_tracer(__name__)
        
        with tracer.start_as_current_span("agno_agent_run") as span:
            span.set_attribute("task", task)
            span.set_attribute("system_prompt", self.system_prompt)
            
            # Ch·∫°y AGNO agent
            response = self.agno_agent.run(
                task,
                system_prompt=self.system_prompt
            )
            
            # Extract result
            result = {
                "answer": response.content,
                "reasoning": getattr(response, "reasoning", ""),
                "tool_calls": getattr(response, "tool_calls", [])
            }
            
            span.set_attribute("answer", result["answer"])
            
            return result
    
    def update_resource(self, resource_name: str, resource_value):
        """
        AgentLightning g·ªçi method n√†y ƒë·ªÉ update prompt sau m·ªói iteration.
        """
        if resource_name == "system_prompt":
            self.system_prompt = resource_value
            print(f"‚úÖ Updated system_prompt to:\n{resource_value}\n")
```

### 5.4 B∆∞·ªõc 3: C·∫•u H√¨nh Training cho APO

```python
# training/config.py (c·∫≠p nh·∫≠t)
from dataclasses import dataclass

@dataclass
class TrainingConfig:
    """C·∫•u h√¨nh training v·ªõi APO."""
    
    # === AgentLightning Settings ===
    otlp_endpoint: str = "http://localhost:4318/v1/traces"
    agent_id: str = "agno-agent-v1"
    n_runners: int = 8  # S·ªë l∆∞·ª£ng parallel runners
    max_iterations: int = 10  # S·ªë iterations cho APO
    
    # === APO Algorithm Settings ===
    algorithm: str = "apo"
    learning_rate: float = 0.001
    
    # Critic LLM (ƒë√°nh gi√° prompt)
    critic_model: str = "gpt-4"
    critic_temperature: float = 0.7
    
    # Editor LLM (s·ª≠a prompt)
    editor_model: str = "gpt-4"
    editor_temperature: float = 0.5
    
    # === Reward Settings ===
    use_llm_grader: bool = True  # D√πng LLM ƒë·ªÉ grade responses
    reward_tolerance: float = 0.1
    
    # === Dataset Settings ===
    task_type: str = "conversation"
    use_real_data: bool = True
    user_data_db_path: str = "core/agno_memory.db"
    max_real_data_age_days: int = 30
```

### 5.5 B∆∞·ªõc 4: Chu·∫©n B·ªã Training Dataset

Quy tr√¨nh chu·∫©n b·ªã d·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa l·ªãch s·ª≠ t∆∞∆°ng t√°c th·ª±c t·∫ø t·ª´ ng∆∞·ªùi d√πng (Real User Data). H·ªá th·ªëng t·ª± ƒë·ªông k·∫øt n·ªëi t·ªõi c∆° s·ªü d·ªØ li·ªáu b·ªô nh·ªõ c·ªßa AGNO (`agno_memory.db`) ƒë·ªÉ tr√≠ch xu·∫•t c√°c phi√™n h·ªôi tho·∫°i g·∫ßn nh·∫•t. M·ªói tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng s·∫Ω ƒë∆∞·ª£c t√°ch th√†nh m·ªôt nhi·ªám v·ª• hu·∫•n luy·ªán (Task) ƒë·ªôc l·∫≠p, lo·∫°i b·ªè c√°c nhi·ªÖu lo·∫°n kh√¥ng c·∫ßn thi·∫øt. Sau ƒë√≥, d·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh t·∫≠p Train v√† Validation (80/20). Vi·ªác s·ª≠ d·ª•ng d·ªØ li·ªáu th·ª±c t·∫ø n√†y gi√∫p thu·∫≠t to√°n APO t·ªëi ∆∞u h√≥a prompt s√°t s∆∞·ªùn v·ªõi nhu c·∫ßu v√† ng·ªØ c·∫£nh c·ª• th·ªÉ m√† ng∆∞·ªùi d√πng th∆∞·ªùng xuy√™n truy v·∫•n, ƒë·∫£m b·∫£o t√≠nh ·ª©ng d·ª•ng cao cho Agent.

**V√≠ d·ª• c·∫•u tr√∫c Task sau khi chuy·ªÉn ƒë·ªïi:**
```json
{
  "question": "L√†m sao ƒë·ªÉ c√†i ƒë·∫∑t AgentLightning?",
  "task_id": "real_session_e9d8f7_msg_2"
}
```

### 5.6 B∆∞·ªõc 5: Setup Training Pipeline

```python
# training/train.py (c·∫≠p nh·∫≠t)
from agentlightning import Trainer
from .agent_wrapper import AgnoLightningAgent

def run_training(training_config, agent_config, db, 
                 train_dataset, val_dataset, store_url=None):
    """Execute APO training."""
    
    # 1. T·∫°o agent wrapper
    agent = AgnoLightningAgent(agent_config)
    
    # 2. Define tunable resources
    resources = {
        "system_prompt": agent.system_prompt  # Initial prompt
    }
    
    # 3. T·∫°o trainer v·ªõi APO algorithm
    trainer = Trainer(
        agent=agent,
        algorithm="apo",
        resources=resources,
        config={
            "critic_model": training_config.critic_model,
            "editor_model": training_config.editor_model,
            "max_iterations": training_config.max_iterations,
            "n_runners": training_config.n_runners
        },
        store_url=store_url  # Optional external store
    )
    
    # 4. Ch·∫°y training
    print("üöÄ Starting APO Training...")
    results = trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset
    )
    
    # 5. L·∫•y optimized prompt
    optimized_prompt = trainer.get_best_resource("system_prompt")
    
    return trainer, optimized_prompt
```

---

## 6. √ù T∆∞·ªüng Training

### 6.1 M·ª•c Ti√™u Training

1. **C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng response**
   - Tr·∫£ l·ªùi ch√≠nh x√°c h∆°n
   - Gi·∫£i th√≠ch r√µ r√†ng h∆°n
   - Reasoning t·ªët h∆°n

2. **T·ªëi ∆∞u h√≥a prompt**
   - T·ª± ƒë·ªông t√¨m prompt t·ªët nh·∫•t
   - Kh√¥ng c·∫ßn manual tuning
   - Adapt theo domain c·ª• th·ªÉ

3. **H·ªçc t·ª´ real user data**
   - S·ª≠ d·ª•ng conversations th·∫≠t t·ª´ users
   - C·∫£i thi·ªán theo feedback th·ª±c t·∫ø

### 6.2 Training Strategy

#### Strategy 1: Domain-Specific Training

```python
# Train cho t·ª´ng domain ri√™ng
domains = ["math", "conversation", "code_generation"]

for domain in domains:
    domain_dataset = filter_by_domain(full_dataset, domain)
    initial_prompt = get_domain_prompt(domain)
    trainer.train(domain_dataset, initial_prompt)
```

#### Strategy 2: Progressive Training

```python
# Training theo ƒë·ªô kh√≥ tƒÉng d·∫ßn
difficulty_levels = ["easy", "medium", "hard"]

current_prompt = initial_prompt
for level in difficulty_levels:
    level_dataset = filter_by_difficulty(full_dataset, level)
    trainer.train(level_dataset, current_prompt)
```

### 6.3 Reward Function Design

```python
# training/engine/grader.py
def calculate_reward(task, agent_response, expected_answer):
    """
    T√≠nh reward cho agent response.
    
    Returns:
        float: Reward t·ª´ 0.0 ƒë·∫øn 1.0
    """
    # 1. Accuracy score (40%)
    accuracy = check_accuracy(agent_response, expected_answer)
    
    # 2. Reasoning quality (60%)
    reasoning = grade_reasoning(agent_response)
    
    return 0.4 * accuracy + 0.6 * reasoning
```

---

## 7. Chu·∫©n B·ªã C√¢u H·ªèi / Dataset

### 7.1 Nguy√™n T·∫Øc Chu·∫©n B·ªã Dataset

#### ‚úÖ DO (N√™n L√†m)

1. **ƒêa d·∫°ng (Diversity)**
   - Cover nhi·ªÅu topic kh√°c nhau
   - Nhi·ªÅu difficulty levels

2. **C√¢n ƒë·ªëi (Balance)**
   - S·ªë l∆∞·ª£ng examples ƒë·ªÅu nhau
   - Tr√°nh bias v·ªÅ m·ªôt lo·∫°i c√¢u h·ªèi

3. **Real-world Representative**
   - Gi·ªëng c√¢u h·ªèi users th·∫≠t s·∫Ω h·ªèi

#### ‚ùå DON'T (Tr√°nh)

1. ‚ùå Dataset qu√° nh·ªè (< 50 examples)
2. ‚ùå Ch·ªâ m·ªôt lo·∫°i c√¢u h·ªèi
3. ‚ùå Expected answers m∆° h·ªì

### 7.2 Template Chu·∫©n B·ªã C√¢u H·ªèi

#### Template 1: Question-Answering

```json
{
  "task": "What is machine learning?",
  "expected_answer": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.",
  "context": "AI/ML basics",
  "difficulty": "medium",
  "category": "definition",
  "requires_reasoning": false
}
```

#### Template 2: Problem Solving

```json
{
  "task": "A train travels 120 km in 2 hours. What is its average speed?",
  "expected_answer": "60 km/h (calculated as 120 km √∑ 2 hours)",
  "context": "Basic math",
  "difficulty": "easy",
  "category": "math",
  "requires_reasoning": true,
  "reasoning_steps": [
    "Identify given: distance = 120 km, time = 2 hours",
    "Apply formula: speed = distance / time",
    "Calculate: 120 / 2 = 60 km/h"
  ]
}
```

#### Template 3: Multi-step Reasoning

```json
{
  "task": "If all roses are flowers, and some flowers fade quickly, can we conclude that some roses fade quickly?",
  "expected_answer": "No, we cannot conclude that. While all roses are flowers, we don't know if roses are among the flowers that fade quickly.",
  "context": "Logic reasoning",
  "difficulty": "hard",
  "category": "logical_reasoning",
  "requires_reasoning": true,
  "reasoning_type": "deductive"
}
```

### 6.3 V√≠ D·ª• Dataset Ho√†n Ch·ªânh

```python
# training_dataset.json
[
    # === Easy Questions (30%) ===
    {
        "task": "What is 2 + 2?",
        "expected_answer": "4",
        "context": "Basic arithmetic",
        "difficulty": "easy",
        "category": "math"
    },
    {
        "task": "What is the capital of Vietnam?",
        "expected_answer": "Hanoi",
        "context": "Geography",
        "difficulty": "easy",
        "category": "factual"
    },
    
    # === Medium Questions (50%) ===
    {
        "task": "Explain the difference between supervised and unsupervised learning.",
        "expected_answer": "Supervised learning uses labeled data to train models, while unsupervised learning finds patterns in unlabeled data.",
        "context": "Machine Learning",
        "difficulty": "medium",
        "category": "explanation"
    },
    {
        "task": "Calculate the area of a circle with radius 5 cm.",
        "expected_answer": "78.54 cm¬≤ (using formula œÄr¬≤ = 3.14159 √ó 5¬≤)",
        "context": "Geometry",
        "difficulty": "medium",
        "category": "math",
        "requires_reasoning": true
    },
    
    # === Hard Questions (20%) ===
    {
        "task": "Analyze the time complexity of a recursive Fibonacci implementation and suggest optimization.",
        "expected_answer": "The recursive approach has O(2^n) time complexity due to redundant calculations. Optimization: use memoization or dynamic programming to achieve O(n).",
        "context": "Computer Science",
        "difficulty": "hard",
        "category": "analysis",
        "requires_reasoning": true
    }
]
```

### 6.4 Tools ƒë·ªÉ T·∫°o Dataset

#### Script T·ª± ƒê·ªông T·∫°o Dataset

```python
# tools/dataset_generator.py
import json
from typing import List, Dict

class DatasetGenerator:
    """Tool ƒë·ªÉ t·∫°o v√† validate training dataset."""
    
    def __init__(self):
        self.dataset = []
    
    def add_question(self, task: str, expected_answer: str,
                     difficulty: str, category: str, **kwargs):
        """Th√™m c√¢u h·ªèi v√†o dataset."""
        question = {
            "task": task,
            "expected_answer": expected_answer,
            "difficulty": difficulty,
            "category": category,
            **kwargs
        }
        self.dataset.append(question)
    
    def validate(self) -> bool:
        """Validate dataset."""
        # Check minimum size
        if len(self.dataset) < 50:
            print("‚ö†Ô∏è Dataset qu√° nh·ªè, n√™n c√≥ √≠t nh·∫•t 50 examples")
            return False
        
        # Check balance
        difficulties = [q["difficulty"] for q in self.dataset]
        easy_pct = difficulties.count("easy") / len(difficulties)
        medium_pct = difficulties.count("medium") / len(difficulties)
        hard_pct = difficulties.count("hard") / len(difficulties)
        
        print(f"Distribution: Easy {easy_pct:.0%}, Medium {medium_pct:.0%}, Hard {hard_pct:.0%}")
        
        return True
    
    def save(self, filepath: str):
        """L∆∞u dataset ra file."""
        if self.validate():
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.dataset, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ Saved {len(self.dataset)} questions to {filepath}")

# Usage
generator = DatasetGenerator()

# Th√™m questions
generator.add_question(
    task="What is Python?",
    expected_answer="Python is a high-level programming language...",
    difficulty="easy",
    category="definition"
)

# Save
generator.save("training_dataset.json")
```

### 6.5 C√°ch L·∫•y Dataset t·ª´ Real User Data

```python
# training/data/extract_from_real_data.py
import sqlite3
from datetime import datetime, timedelta

def extract_user_conversations(db_path: str, days: int = 30) -> List[Dict]:
    """
    Extract conversations t·ª´ AGNO memory database.
    
    Args:
        db_path: Path to agno_memory.db
        days: L·∫•y data t·ª´ bao nhi√™u ng√†y g·∫ßn ƒë√¢y
    
    Returns:
        List of training examples
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # L·∫•y conversations g·∫ßn ƒë√¢y
    cutoff_date = datetime.now() - timedelta(days=days)
    
    query = """
    SELECT user_message, agent_response, timestamp, rating
    FROM conversations
    WHERE timestamp > ?
    AND rating >= 4  -- Ch·ªâ l·∫•y conversations c√≥ rating t·ªët
    ORDER BY timestamp DESC
    """
    
    cursor.execute(query, (cutoff_date,))
    rows = cursor.fetchall()
    
    dataset = []
    for row in rows:
        user_msg, agent_resp, timestamp, rating = row
        dataset.append({
            "task": user_msg,
            "expected_answer": agent_resp,
            "context": "real_user_conversation",
            "difficulty": "medium",  # Can be inferred
            "category": "conversation",
            "timestamp": timestamp,
            "user_rating": rating
        })
    
    conn.close()
    
    print(f"‚úÖ Extracted {len(dataset)} conversations from last {days} days")
    return dataset
```

---



---


## 8. Quy Tr√¨nh Training ƒê·∫ßy ƒê·ªß

### 8.1 Checklist Tr∆∞·ªõc Khi Training

- [ ] Environment variables ƒë√£ setup (OPENAI_API_KEY, etc.)
- [ ] Dependencies ƒë√£ c√†i ƒë·∫∑t
- [ ] Training dataset ƒë√£ chu·∫©n b·ªã (‚â• 50 examples)
- [ ] Validation dataset ƒë√£ chu·∫©n b·ªã (‚â• 20 examples)
- [ ] Initial prompt ƒë√£ vi·∫øt
- [ ] Reward function ƒë√£ config
- [ ] Training config ƒë√£ review

### 8.2 Command ƒë·ªÉ Ch·∫°y Training

```bash
# Dry run (test setup)
python -m training.train --dry-run

# Training v·ªõi default config
python -m training.train --iterations 10 --algorithm apo

# Training v·ªõi custom config
python -m training.train \
    --iterations 20 \
    --algorithm apo \
    --real-data-db "core/agno_memory.db"

# Training v·ªõi external store (dashboard)
python -m training.train \
    --iterations 10 \
    --algorithm apo \
    --store-url "http://localhost:4747"
```

### 8.3 Monitoring Training

```python
# Training output s·∫Ω c√≥ d·∫°ng:
"""
üöÄ Starting APO Training...

Iteration 1/10:
  Current Prompt: "You are a helpful assistant."
  Avg Reward: 0.65
  
Iteration 2/10:
  Critic: "Prompt lacks specificity..."
  Editor: Adding step-by-step guidance
  Updated Prompt: "You are a helpful assistant. Think carefully..."
  Avg Reward: 0.72 ‚Üë
  
...

Iteration 10/10:
  Avg Reward: 0.89 ‚Üë
  
‚úÖ Training Complete!
"""
```

### 8.4 Sau Training

```python
# 1. Load optimized prompt t·ª´ prompt manager
from training.utils.prompt_manager import PromptManager
pm = PromptManager()
best_record = pm.load_best_prompt()
optimized_prompt = best_record.prompt_text

# 2. Update agent v·ªõi prompt m·ªõi
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions=optimized_prompt.split("\n"),
    # ... c√°c config kh√°c
)
```

---

## 9. Best Practices & Tips

### 9.1 Training Tips

1. **Start Simple**
   - B·∫Øt ƒë·∫ßu v·ªõi prompt ƒë∆°n gi·∫£n
   - Dataset nh·ªè (~10 examples) ƒë·ªÉ test setup
   - √çt iterations (~2-3) ƒë·ªÉ validate reward logic

2. **Iterate Gradually**
   - TƒÉng d·∫ßn dataset size khi h·ªá th·ªëng ƒë√£ ·ªïn ƒë·ªãnh
   - Monitor reward curves tr√™n dashboard
   - Ki·ªÉm tra xem prompt sinh ra c√≥ b·ªã "l·∫∑p" hay kh√¥ng

3. **Diversity Matters**
   - Dataset n√™n bao g·ªìm c·∫£ nh·ªØng tr∆∞·ªùng h·ª£p Agent th∆∞·ªùng tr·∫£ l·ªùi sai
   - ƒê·∫£m b·∫£o c√¢u h·ªèi bao qu√°t c√°c t√¨nh hu·ªëng th·ª±c t·∫ø m√† User hay g·∫∑p

4. **Reward Engineering**
   - N·∫øu d√πng LLM Grader, h√£y ƒë·∫£m b·∫£o m·∫´u ch·∫•m ƒëi·ªÉm (Grading Prompt) th·∫≠t r√µ r√†ng
   - Reward 0.0 c√≥ th·ªÉ do l·ªói runtime ho·∫∑c Agent kh√¥ng tr·∫£ l·ªùi ƒë√∫ng format

### 9.2 Common Pitfalls

‚ùå **Tr√°nh nh·ªØng l·ªói n√†y:**

1. **Overfitting**
   - Dataset qu√° nh·ªè nh∆∞ng iterations qu√° cao d·∫´n ƒë·∫øn prompt b·ªã t·ªëi ∆∞u h√≥a qu√° m·ª©c cho v√†i c√¢u h·ªèi c·ª• th·ªÉ.

2. **Poor Reward Signal**
   - Grader qu√° l·ªèng l·∫ªo (lu√¥n cho 1.0) ho·∫∑c qu√° kh·∫Øt khe (lu√¥n cho 0.0).

3. **Infrastructure Issues**
   - OTLP endpoint kh√¥ng ch·∫°y khi·∫øn AgentLightning kh√¥ng thu th·∫≠p ƒë∆∞·ª£c traces ƒë·ªÉ ph√¢n t√≠ch l·ªói.

### 9.3 Debugging Guide

```python
# Xem dashboard ƒë·ªÉ debug traces
# agl store --port 4747

# Ki·ªÉm tra logs chi ti·∫øt
# tail -f training/logs/training.log

# Ki·ªÉm tra database real data
# python -m training.data.data_preparation (ch·∫°y script l·∫ª)
```

---

## 10. T√†i Nguy√™n Tham Kh·∫£o

### 10.1 Documentation

- [AgentLightning Docs](https://microsoft.github.io/agentlightning/)
- [AGNO Docs](https://docs.agno.com/)
- [APO Paper](https://arxiv.org/abs/2309.03409)
- [OpenTelemetry Tracing](https://opentelemetry.io/docs/)

### 10.2 C√¥ng c·ª• h·ªó tr·ª£
- **AgentOps**: ƒê·ªÉ quan s√°t (observability) n√¢ng cao trong production.
- **SQLite Browser**: ƒê·ªÉ ki·ªÉm tra `agno_memory.db`.

---

## 11. K·∫øt Lu·∫≠n

### 11.1 T√≥m T·∫Øt

T√≠ch h·ª£p **AgentLightning** mang l·∫°i kh·∫£ nƒÉng **t·ª± ƒë·ªông h√≥a ho√†n to√†n** vi·ªác vi·∫øt prompt. Thay v√¨ "ƒëo√°n" xem prompt n√†o t·ªët, h·ªá th·ªëng s·∫Ω t·ª± th·ª≠ nghi·ªám, t·ª± soi l·ªói qua **OpenTelemetry Traces** v√† t·ª± s·ª≠a m√¨nh.

### 11.2 Th√†nh Ph·∫©m ƒê·∫°t ƒê∆∞·ª£c
- H·ªá th·ªëng tr√≠ch xu·∫•t d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ AGNO.
- Quy tr√¨nh ch·∫•m ƒëi·ªÉm linh ho·∫°t kh√¥ng c·∫ßn ƒë√°p √°n m·∫´u.
- C∆° ch·∫ø l∆∞u tr·ªØ l·ªãch s·ª≠ prompts v√† k·∫øt qu·∫£ training r√µ r√†ng.
- T·∫≠n d·ª•ng s·ª©c m·∫°nh c·ªßa distributed tracing ƒë·ªÉ ph√¢n t√≠ch l·ªói s√¢u.

**H·ªá th·ªëng hi·ªán ƒë√£ s·∫µn s√†ng cho giai ƒëo·∫°n hu·∫•n luy·ªán quy m√¥ l·ªõn.** üöÄ
